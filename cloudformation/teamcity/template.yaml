AWSTemplateFormatVersion: 2010-09-09

Description:

  This template hosts TeamCity on an ec2 instance in the default vpc.

Parameters:

  InstanceType:
    Type: String
    AllowedValues:
      - "t2.medium"
    Description: EC2 Instance Type... t2.micro, m1.small, m1.large, etc...

  ImageId:
    Type: String
    Description: EC2 where TeamCity server and agent will run is created from this base image

  KeyPairName:
    Type: String
    Description: use this keypair to ssh into the ec2

  EC2HostPort:
    Type: Number
    Description: tcp port for ec2 incoming requests

  SecretsManagerName:
    Type: String
    Description: the name of the teamcity secrets json object in aws secrets manager

  PostgresPasswordKey:
    Type: String
    Description: the name of the postgres password key in aws secrets manager

  DNSHostedZoneId:
    Type: String
    Description: the id of the existing aws public hosted zone in route53 where the dns record already exists

  DNSRecordName:
    Type: String
    Description: the name of the dns record that already exists in route53

  DNSRecordTTL:
    Type: String
    Description: the ttl of the dns record that already exists in route53

  AllowAccessCidrIp:
    Type: String
    Description: the security group for the ec2 will only allow incoming connections from this cidr

  PostgresDockerImage:
    Type: String
    Description: path to pull down the postgres docker image

  TeamCityServerDockerImage:
    Type: String
    Description: path to pull down the teamcity server docker image

  TeamCityAgentDockerImage:
    Type: String
    Description: path to pull down the teamcity agent docker image

  RoleName:
    Type: String
    Default: "sq-teamcity-ec2-role"
    Description: uniqe name for the iam role

  SecurityGroupName:
    Type: String
    Default: "sq-teamcity-ec2-security-group"
    Description: uniqe name for the ec2 security group

Resources:

  # EC2 Instance Configuration
  # Note : The cfn-init helper script processes these configuration sections in the following order: packages, groups, users, sources, files, commands, and then services. If you require a different order, separate your sections into different config keys, and then use a configset that specifies the order in which the config keys should be processed.

  myEC2Instance:
    Type: AWS::EC2::Instance
    DependsOn: myEC2SecurityGroup
    Metadata:
      AWS::CloudFormation::Init:

        configSets:
          configs:
            - yum-update
            - package-installs
            - docker-service-start
            - bootstrap-teamcity-server
            - docker-compose

        yum-update:
          commands:
            command-1:
              command: "yum update -y;"

        package-installs:
          packages:
            yum:
              docker: [ ]
              jq: [ ]

        docker-service-start:
          services:
            systemd:
              docker:
                enabled: "true"
                ensureRunning: "true"

        bootstrap-teamcity-server:
          files:
            /working-directory/docker-compose.tmpl:
              mode: "000644"
              content: !Sub |
                version: '3.1'

                # ./buildserver_pgdata - Posgres DB data
                # ./data_dir - TeamCity data directory
                # ./teamcity-server-logs - logs of primary TeamCity server
                # ./agents/agent-1/conf - configuration directory for the first build agent
                # ./agents/agent-2/conf - configuration directory for the second build agent

                services:
                  db:
                    container_name: teamcity-db
                    image: ${PostgresDockerImage}
                    user: root
                    restart: always
                    environment:
                      - POSTGRES_USER=teamcity_user
                      - POSTGRES_PASSWORD={{${PostgresPasswordKey}}}
                      - POSTGRES_DB=teamcity_db
                      - PG_DATA=/var/lib/postgresql/data
                    volumes:
                      - ./buildserver_pgdata:/var/lib/postgresql/data
                    ports:
                      - 5433:5432

                  teamcity:
                    container_name: teamcity-server
                    image: ${TeamCityServerDockerImage}
                    user: root
                    restart: always
                    ports:
                      - "80:8111"
                    volumes:
                      - ./data_dir:/data/teamcity_server/datadir
                      - ./teamcity-server-logs:/opt/teamcity/logs
                    depends_on:
                      - db

                  teamcity-agent-1:
                    container_name: teamcity-agent-1
                    image: ${TeamCityAgentDockerImage}
                    user: root
                    restart: always
                    privileged: true
                    volumes:
                      - ./agents/agent-1/conf:/data/teamcity_agent/conf
                    environment:
                      - DOCKER_IN_DOCKER=start
                      - SERVER_URL=http://teamcity:8111

                  teamcity-agent-2:
                    container_name: teamcity-agent-2
                    image: ${TeamCityAgentDockerImage}
                    user: root
                    restart: always
                    privileged: true
                    volumes:
                      - ./agents/agent-2/conf:/data/teamcity_agent/conf
                    environment:
                      - DOCKER_IN_DOCKER=start
                      - SERVER_URL=http://teamcity:8111

            /working-directory/update-dns-record.tmpl:
              mode: "000644"
              content: !Sub |

                {
                  "Comment": "updating dns record because the teamcity ec2 has been replaced and is launching now (public ip address has changed)",
                  "Changes": [
                    {
                      "Action": "UPSERT",
                      "ResourceRecordSet": {
                        "Name": "${DNSRecordName}",
                        "Type": "A",
                        "TTL": ${DNSRecordTTL},
                        "ResourceRecords": [
                          {
                            "Value": "{{PUBLIC_IP}}"
                          }
                        ]
                      }
                    }
                  ]
                }

            /working-directory/bootstrap.sh:
              mode: "000744"
              content: !Sub |
                #!/bin/bash

                # install docker-compose
                curl -L https://github.com/docker/compose/releases/download/1.22.0/docker-compose-$(uname -s)-$(uname -m) -o /usr/bin/docker-compose;
                chmod 755 /usr/bin/docker-compose;

                # install mustache
                curl -L https://git.io/get-mo -o /usr/bin/mo;
                chmod 755 /usr/bin/mo;

                # get the public ip of this ec2 and save it to an env var for mustache
                export PUBLIC_IP=$(curl http://169.254.169.254/latest/meta-data/public-ipv4);

                # mustache render to update update-dns-record.json (adding public ip of this ec2 to the json file)
                cat ./update-dns-record.tmpl | mo | tee ./update-dns-record.json;

                # update the existing dns record in route53 because the teamcity ec2 has been replaced (public ip address has changed)
                aws route53 change-resource-record-sets --hosted-zone-id ${DNSHostedZoneId} --change-batch file://./update-dns-record.json;

                # get the postgres password and save it to an env var for mustache
                export ${PostgresPasswordKey}=$(aws secretsmanager get-secret-value --region ${AWS::Region} --secret-id ${SecretsManagerName} | jq --raw-output '.SecretString' | jq -r .${PostgresPasswordKey});

                # mustache render to update docker-compose.yml (adding postgres password to the yml file)
                cat ./docker-compose.tmpl | mo | tee ./docker-compose.yml;

            /var/lib/cloud/scripts/per-boot/reboot.sh:
              mode: "000744"
              content: !Sub |
                #!/bin/bash

                # we are adding this file to the per-boot directory to make our bootstrap.sh script run on every ec2 reboot
                # we do this to keep the route53 dns record updated because the ec2 public ip changes on an ec2 on reboot
                # the other stuff in the bootstrap script will run too, but no big deal, it's all idempotent

                cd /working-directory && ./bootstrap.sh;

          commands:
            command-1:
              command: "./bootstrap.sh"
              cwd: "/working-directory"

        docker-compose:
          commands:
            command-1:
              command: "docker-compose up -d;"
              cwd: "/working-directory"

    Properties:
      InstanceType: !Ref InstanceType
      ImageId: !Ref ImageId
      KeyName: !Ref KeyPairName
      IamInstanceProfile: !Ref myInstanceProfile
      SecurityGroups:
        - !Ref SecurityGroupName
      BlockDeviceMappings:
        - DeviceName: "/dev/xvda"
          Ebs:
            VolumeType: "gp2"
            VolumeSize: 20
            DeleteOnTermination: true
            Encrypted: false
      UserData:
        'Fn::Base64':
          !Sub |
          #!bin/bash -xe

          /opt/aws/bin/cfn-init -v \
            --region ${AWS::Region} \
            --stack ${AWS::StackName} \
            --resource myEC2Instance \
            --configsets configs;
      Tags:
        - Key: Name
          Value: "sq-teamcity-ec2"

  myInstanceProfile:
    Type: AWS::IAM::InstanceProfile
    DependsOn: myRole
    Properties:
      Roles:
        - !Ref RoleName

  myRole:
    Type: "AWS::IAM::Role"
    Properties:
      RoleName: !Ref RoleName
      ManagedPolicyArns:
        - "arn:aws:iam::aws:policy/SecretsManagerReadWrite"
        - "arn:aws:iam::aws:policy/AmazonRoute53FullAccess"
        - "arn:aws:iam::aws:policy/AWSCloudFormationFullAccess"
        - "arn:aws:iam::aws:policy/AmazonEC2FullAccess"
        - "arn:aws:iam::aws:policy/IAMFullAccess"
      Policies:
        - PolicyName: "sq-teamcity-iam-policy"
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - eks:CreateCluster
                  - eks:DeleteCluster
                  - eks:DescribeCluster
                Resource: "*"
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: "Allow"
            Principal:
              Service:
                - "ec2.amazonaws.com"
            Action:
              - "sts:AssumeRole"
      Path: "/"

  myEC2SecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: "SG for TeamCity EC2 instances"
      GroupName: !Ref SecurityGroupName
      SecurityGroupIngress:
        - Description: "allow incoming ssh connections"
          IpProtocol: "tcp"
          FromPort: 22
          ToPort: 22
          CidrIp: !Ref AllowAccessCidrIp # allow ssh connections from this cidr
        - Description: "allow incoming http connections"
          IpProtocol: "tcp"
          FromPort: !Ref EC2HostPort
          ToPort: !Ref EC2HostPort
          CidrIp: !Ref AllowAccessCidrIp # allow ssh connections from this cidr
      Tags:
        - Key: Name
          Value: !Ref SecurityGroupName

Outputs:

    PublicDNS:
      Description:
        The EC2 Instance DNS Path
      Value:
        !GetAtt
        - myEC2Instance
        - PublicDnsName
      Export:
        Name:  !Sub "${AWS::StackName}-public-dns"

    PublicIp:
      Description:
        The EC2 Instance Public IP Address
      Value:
        !GetAtt
        - myEC2Instance
        - PublicIp
      Export:
        Name:  !Sub "${AWS::StackName}-public-ip"

    RoleName:
      Description:
        The Name of the EC2 Instance Profile IAM Role
      Value:
        !Ref RoleName
      Export:
        Name: "sq-teamcity-role-name"